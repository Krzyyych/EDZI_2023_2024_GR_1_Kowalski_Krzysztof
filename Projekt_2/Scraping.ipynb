{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d388a3-3cf9-455a-8011-03e63d6f44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib.robotparser\n",
    "from itertools import chain\n",
    "import json\n",
    "#from gensim.summarization import summarize problem z instalacja wiec innego\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48b90df-2217-498a-a474-6d5df416e060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_text(text, language, sentences_count = 10):\n",
    "    without_html = re.sub(re.compile('<.*?>'), '', str(text)).replace('\"', '-') \n",
    "    parser = PlaintextParser.from_string(without_html, Tokenizer(language))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count)\n",
    "    return ' '.join([str(sentence) for sentence in summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcec470c-eed4-4300-8687-8d0ef6038026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_location(company_name):\n",
    "        # Base URL for Nominatim API\n",
    "        base_url = \"https://nominatim.openstreetmap.org/search\"\n",
    "        # Parameters for the search query\n",
    "        params = {\n",
    "            \"q\": company_name,\n",
    "            \"format\": \"json\",\n",
    "            \"addressdetails\": 1  # Include address details in the response\n",
    "        }\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36\"\n",
    "        }\n",
    "        # Make the request to Nominatim API\n",
    "        response = requests.get(base_url, params = params, headers = headers)\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if any results were found\n",
    "        if data:\n",
    "            # Extract location information from the first result\n",
    "            location = {\n",
    "                \"display_name\": data[0][\"display_name\"]\n",
    "            }\n",
    "            return location\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e703a0-8ec0-495f-9cf2-1a47fa7b184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def it_pracuj():\n",
    "    links_jump = []\n",
    "    url_page = \"https://it.pracuj.pl/praca/krakow;wp?rd=0&et=17%2C4%2C18&sal=1&its=big-data-science\"\n",
    "    response_page = requests.get(url_page)\n",
    "    soup_page = BeautifulSoup(response_page.text, 'html.parser')\n",
    "    pages = soup_page.select('.listing_n1mxvncp.listing_n1mxvncp')\n",
    "    if pages:\n",
    "        pages_number = 2\n",
    "    else:\n",
    "        pages_number = 1\n",
    "    for i in range(1, pages_number + 1):\n",
    "        url = \"https://it.pracuj.pl/praca/krakow;wp?rd=0&et=17%2C4%2C18&sal=1&its=big-data-science&pn=\" + str(i) \n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            pracuj_header = soup.select('.c1fljezf .core_n194fgoq')\n",
    "            links = [link['href'] for link in pracuj_header if 'href' in link.attrs]\n",
    "            pattern = re.compile(r'^https://www.pracuj.pl/praca/')\n",
    "            filtered_links = [link for link in links if pattern.match(link)]\n",
    "            links_jump.extend(filtered_links)\n",
    "        else:\n",
    "            print(\"Failed to it.pracuj.pl\")\n",
    "            return None\n",
    "    links_jump = list(set(links_jump))\n",
    "    #print(links_jump)\n",
    "    return jump_to_link_pracuj(links_jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846c15a9-75e4-481c-bcbb-be9ae1a44509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jump_to_link_pracuj(links):\n",
    "    jobs_data = []\n",
    "    source = \"it.pracuj.pl\"\n",
    "    category = \"BigData/Data Science\"\n",
    "    currency = \"PLN\"\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            position = soup.select('.offer-viewzJYJpV .offer-viewkHIhn3')[0].contents[0]\n",
    "\n",
    "            company = soup.select('.offer-viewwtdXJ4')[0].contents[0]\n",
    "            \n",
    "            kind = soup.select('.offer-viewSGW6Yi')[0].contents[0]\n",
    "            mth_or_h = re.search(r'\\b(hr|mth|mies|godz)\\.$', kind).group(1)\n",
    "            gross_or_net = re.search(r'\\b(brutto|gross|netto|net)\\b', kind).group(1)\n",
    "            \n",
    "            min_salary = soup.select('.offer-viewZGJhIB')\n",
    "            if min_salary:\n",
    "                min_salary = min_salary[0].contents[0]\n",
    "                min_salary = re.sub(r'[^\\d,.]', '', min_salary)\n",
    "                min_salary = min_salary.replace(',', '.')\n",
    "                min_salary = min_salary.split('.')[0]\n",
    "                min_salary = int(min_salary)\n",
    "                max_salary = soup.select('.offer-viewYo2KTr')[0].contents[0]\n",
    "                max_salary = re.sub(r'[^\\d,.]', '', max_salary)\n",
    "                max_salary = max_salary.replace(',', '.')\n",
    "                max_salary = max_salary.split('.')[0]\n",
    "                max_salary = float(max_salary)\n",
    "                if mth_or_h in ['godz', 'hr']:\n",
    "                    if gross_or_net in ['netto', 'net']:\n",
    "                        min_salary *= 168 * 1.23\n",
    "                        max_salary *= 168 * 1.23\n",
    "                    else:\n",
    "                        min_salary *= 168\n",
    "                        max_salary *= 168\n",
    "                else:\n",
    "                    if gross_or_net in ['netto', 'net']:\n",
    "                        min_salary *= 1.23\n",
    "                        max_salary *= 1.23\n",
    "                min_salary = round(min_salary)\n",
    "                max_salary = round(max_salary)\n",
    "            else:\n",
    "                min_salary_element = soup.select('.offer-viewYo2KTr')\n",
    "                if min_salary_element:\n",
    "                    min_salary = min_salary_element[0].contents[0]\n",
    "                else:\n",
    "                    min_salary = None\n",
    "                    max_salary = None\n",
    "                max_salary = None\n",
    "\n",
    "            seniority_str = soup.select('.offer-viewXo2dpV')[2].contents[0]\n",
    "            if ',' in seniority_str:  \n",
    "                if 'ekspert' in seniority_str: \n",
    "                    seniority = seniority_str.split(',')[0]\n",
    "                else:\n",
    "                    seniority = seniority_str.split(',')[1]\n",
    "            else:  \n",
    "                seniority = seniority_str\n",
    "\n",
    "            seniority = seniority.replace(\"starszy specjalista (Senior)\", \"Senior\").replace(\"specjalista (Mid / Regular)\", \"Mid\").replace(\"junior specialist (Junior)\", \"Junior\")\n",
    "            seniority = seniority.replace('senior specialist (Senior)', \"Senior\").replace(\" senior specialist (Senior)\", \"Senior\").replace(\" mÅ\\x82odszy specjalista (Junior)\", \"Junior\").replace('specialist (Mid / Regular)', 'Mid')\n",
    "            skills_all = soup.select(\".offer-viewfjH4z3:first-of-type .offer-viewU0gxPf\")\n",
    "            skills = [skill.text.strip() for skill in skills_all]\n",
    "            job_data = {\n",
    "                \"Źródło\": source,\n",
    "                \"Link\": link,\n",
    "                \"Pozycja\": position,\n",
    "                \"Firma\": company,\n",
    "                \"Min salary\": min_salary,\n",
    "                \"Max salary\": max_salary,\n",
    "                \"Currency\": currency,\n",
    "                \"Skills\": skills,\n",
    "                \"Category\": category,\n",
    "                \"Seniority\": seniority,\n",
    "                \"Adres\": get_location(company),\n",
    "                \"Podsumowanie\": summarize_text(soup, language = 'English'), \n",
    "            }\n",
    "            jobs_data.append(job_data)\n",
    "        else:\n",
    "            print(f\"Nie można pobrać strony: {link}\")\n",
    "    return jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa4709e-d693-4c0d-9401-c368fcb1ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def justjoin_it():\n",
    "    url = \"https://justjoin.it/krakow/data/experience-level_junior.mid.senior/with-salary_yes\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        justjoin_header = soup.select('.css-4lqp8g ')\n",
    "        links = [link['href'] for link in justjoin_header if 'href' in link.attrs]\n",
    "        links = [\"https://justjoin.it\" + link for link in links]\n",
    "    else:\n",
    "        print(\"Failed to it.pracuj.pl\")\n",
    "        return None\n",
    "    return jump_to_link_justjoin(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa433557-083f-43aa-930e-9cff3ccde5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jump_to_link_justjoin(links):\n",
    "    jobs_data = []\n",
    "    source = \"justjoin.it\"\n",
    "    category = \"Data\"\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            position = soup.select('.css-wx6bq4')[0].contents[-1].text\n",
    "            \n",
    "            company = soup.select('.css-u51ts9>*:not(:last-of-type)')[0].contents[1]\n",
    "            \n",
    "            seniority = soup.select('.css-15wyzmd')[1].contents[0]\n",
    "            seniority = seniority.replace(\" Senior\", \"Senior\")\n",
    "\n",
    "            min_salary = float(soup.select('.css-1pavfqb')[0].contents[0].text.replace(\" \", \"\")) \n",
    "            max_salary = float(soup.select('.css-1pavfqb')[0].contents[2].text.replace(\" \", \"\"))\n",
    "            currency = soup.select('.css-1pavfqb')[0].contents[5]\n",
    "\n",
    "            skills_all = soup.select('.css-x1xnx3')\n",
    "            skills = [skill.text.strip() for skill in skills_all]\n",
    "            job_data = {\n",
    "                \"Źródło\": source,\n",
    "                \"Link\": link,\n",
    "                \"Pozycja\": position,\n",
    "                \"Firma\": company,\n",
    "                \"Min salary\": min_salary,\n",
    "                \"Max salary\": max_salary,\n",
    "                \"Currency\": currency,\n",
    "                \"Skills\": skills,\n",
    "                \"Category\": category,\n",
    "                \"Seniority\": seniority,\n",
    "                \"Adres\": get_location(company),\n",
    "                \"Podsumowanie\": summarize_text(soup, language = 'English')\n",
    "                \n",
    "            }\n",
    "            jobs_data.append(job_data)\n",
    "        else:\n",
    "            print(f\"Nie można pobrać strony: {link}\")\n",
    "    return jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66501774-4d5a-4e89-97ce-de18b734cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pracuj = it_pracuj()\n",
    "    justjoin = justjoin_it()\n",
    "    if pracuj is not None and justjoin is not None:\n",
    "        all_jobs = pracuj + justjoin\n",
    "        for idx, job in enumerate(all_jobs, start = 1):\n",
    "            job[\"Id\"] = idx\n",
    "        \n",
    "        with open('jobs_data.json', 'w', encoding = 'utf-8') as f:\n",
    "            json.dump(all_jobs, f, indent = 4, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "604aea41-0ba7-45b4-a5b7-17346ab0cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27878e6e-380c-4195-91d6-5333b33aaa68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
